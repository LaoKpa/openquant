{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Labeling for bet side and size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Triple-barrier method for determining side and Meta-labeling for size of the bet. Meta-labeling is a technique introduced by Marco Lopez De Prado in Advances to Financial machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# standard imports\n",
    "from pathlib import PurePath, Path \n",
    "import sys\n",
    "import time\n",
    "from collections import OrderedDict as od \n",
    "import re \n",
    "import os \n",
    "import json \n",
    "\n",
    "# scientific stack\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "import mlfinlab as ml\n",
    "import sklearn as sk\n",
    "\n",
    "# visual tools and plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            open     high      low    close  cum_vol  \\\n",
      "date_time                                                              \n",
      "2011-07-31 23:31:58.810  1306.00  1308.75  1301.75  1305.75    53658   \n",
      "2011-08-01 02:55:17.443  1305.75  1309.50  1304.00  1306.50    53552   \n",
      "2011-08-01 07:25:56.319  1306.75  1309.75  1304.75  1305.00    53543   \n",
      "2011-08-01 08:33:10.903  1305.00  1305.00  1299.00  1300.00    53830   \n",
      "2011-08-01 10:51:41.842  1300.00  1307.75  1299.00  1307.75    53734   \n",
      "...                          ...      ...      ...      ...      ...   \n",
      "2012-07-30 12:30:28.642  1379.25  1380.00  1377.50  1377.75    50843   \n",
      "2012-07-30 13:29:21.258  1377.75  1380.00  1377.00  1379.25    50782   \n",
      "2012-07-30 13:35:05.407  1379.25  1383.25  1379.00  1382.50    50675   \n",
      "2012-07-30 13:43:43.711  1382.50  1383.25  1380.00  1381.00    50667   \n",
      "2012-07-30 13:54:26.158  1380.75  1381.75  1379.75  1380.75    50698   \n",
      "\n",
      "                          cum_dollar  cum_ticks  \n",
      "date_time                                        \n",
      "2011-07-31 23:31:58.810  70035704.75      14115  \n",
      "2011-08-01 02:55:17.443  70006277.00      15422  \n",
      "2011-08-01 07:25:56.319  70000901.00      14727  \n",
      "2011-08-01 08:33:10.903  70094217.75      14987  \n",
      "2011-08-01 10:51:41.842  70033006.25      14499  \n",
      "...                              ...        ...  \n",
      "2012-07-30 12:30:28.642  70116589.50      17923  \n",
      "2012-07-30 13:29:21.258  70014483.25      14040  \n",
      "2012-07-30 13:35:05.407  70001889.25      12017  \n",
      "2012-07-30 13:43:43.711  70002243.75      13904  \n",
      "2012-07-30 13:54:26.158  70007575.25      12706  \n",
      "\n",
      "[10000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# read in and store raw tick data in pandas dataframe to be cleaned and transformed\n",
    "raw_tick_data = pd.read_csv('tick_data.csv')\n",
    "raw_tick_data['date_time'] = pd.to_datetime(raw_tick_data.date_time)\n",
    "raw_tick_data.set_index('date_time', drop=True, inplace=True)\n",
    "print(raw_tick_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dollar Bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we transform the raw ohlc data into various financial data structures that provide a better representation of the movement of trade information throughout the market.  These bars include: dollar, tick, and volume bars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Primary Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is is the primary model that we will use to drive our strategy (EMA Crossover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_window = 12\n",
    "slow_window = 26\n",
    "\n",
    "raw_tick_data['fast_mavg'] = raw_tick_data['close'].rolling(window=fast_window, min_periods=fast_window, center=False).mean()\n",
    "raw_tick_data['slow_mavg'] = raw_tick_data['close'].rolling(window=slow_window, min_periods=slow_window, center=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_time\n",
      "2011-07-31 23:31:58.810    False\n",
      "2011-08-01 02:55:17.443    False\n",
      "2011-08-01 07:25:56.319    False\n",
      "2011-08-01 08:33:10.903    False\n",
      "2011-08-01 10:51:41.842    False\n",
      "                           ...  \n",
      "2012-07-30 12:30:28.642     True\n",
      "2012-07-30 13:29:21.258    False\n",
      "2012-07-30 13:35:05.407    False\n",
      "2012-07-30 13:43:43.711    False\n",
      "2012-07-30 13:54:26.158    False\n",
      "Length: 10000, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "raw_tick_data['side'] = np.nan\n",
    "long_signals = raw_tick_data['fast_mavg'] >= raw_tick_data['slow_mavg']\n",
    "short_signals = raw_tick_data['fast_mavg'] < raw_tick_data['slow_mavg']\n",
    "raw_tick_data.loc[long_signals, 'side'] = 1\n",
    "raw_tick_data.loc[short_signals, 'side'] = -1\n",
    "data = raw_tick_data.dropna()\n",
    "raw_data = data.copy()\n",
    "print(long_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_time\n",
      "2011-08-02 14:49:14.665         NaN\n",
      "2011-08-02 14:55:22.761    0.000009\n",
      "2011-08-02 15:02:12.277    0.000013\n",
      "2011-08-02 15:07:17.932    0.001585\n",
      "2011-08-02 15:12:42.641    0.001509\n",
      "                             ...   \n",
      "2012-07-30 12:30:28.642    0.008794\n",
      "2012-07-30 13:29:21.258    0.009193\n",
      "2012-07-30 13:35:05.407    0.009381\n",
      "2012-07-30 13:43:43.711    0.009588\n",
      "2012-07-30 13:54:26.158    0.009764\n",
      "Name: close, Length: 9926, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# set daily volatility\n",
    "daily_vol = ml.util.get_daily_vol(close=data['close'], lookback=50)\n",
    "print(daily_vol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_time\n",
      "2011-08-02 14:49:14.665             NaN\n",
      "2011-08-02 14:55:22.761    2.095284e-07\n",
      "2011-08-02 15:02:12.277    2.818900e-07\n",
      "2011-08-02 15:07:17.932    3.530381e-05\n",
      "2011-08-02 15:12:42.641    3.359834e-05\n",
      "                               ...     \n",
      "2012-07-30 12:30:28.642    1.958661e-04\n",
      "2012-07-30 13:29:21.258    2.047418e-04\n",
      "2012-07-30 13:35:05.407    2.089223e-04\n",
      "2012-07-30 13:43:43.711    2.135460e-04\n",
      "2012-07-30 13:54:26.158    2.174610e-04\n",
      "Name: close, Length: 9926, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# convert from daily vol to hourly vol (since our data in hourly)\n",
    "trading_hours_in_day = 8\n",
    "trading_days_in_year = 252\n",
    "hourly_vol = daily_vol / math.sqrt(trading_hours_in_day * trading_days_in_year)\n",
    "hourly_vol_mean = hourly_vol.mean()\n",
    "print(hourly_vol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply CUSUM Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the CUSUM filter is to locate instances of change detection.  These are points that will later be used as t_events for the Triple-barrier method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2011-08-01 14:54:45.324000', '2011-08-01 15:01:17.292000',\n",
       "               '2011-08-01 15:05:30.877000', '2011-08-01 15:11:41.236000',\n",
       "               '2011-08-01 15:26:40.200000', '2011-08-01 15:34:37.446000',\n",
       "               '2011-08-01 15:38:23.090000', '2011-08-01 15:56:25.240000',\n",
       "               '2011-08-01 16:16:58.143000', '2011-08-01 16:29:55.276000',\n",
       "               ...\n",
       "               '2012-07-27 20:01:25.794000', '2012-07-27 20:14:35.480000',\n",
       "               '2012-07-30 06:13:28.136000', '2012-07-30 08:10:13.266000',\n",
       "               '2012-07-30 10:16:04.616000', '2012-07-30 12:30:28.642000',\n",
       "               '2012-07-30 13:29:21.258000', '2012-07-30 13:35:05.407000',\n",
       "               '2012-07-30 13:43:43.711000', '2012-07-30 13:54:26.158000'],\n",
       "              dtype='datetime64[ns]', length=9523, freq=None)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply symetric CUSUM filter and get timestamps for events\n",
    "cusum_events = ml.filters.cusum_filter(data['close'], threshold=hourly_vol_mean * 0.5)\n",
    "cusum_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2011-08-01 14:54:45.324   2011-08-02 14:55:22.761\n",
       "2011-08-01 15:01:17.292   2011-08-02 15:02:12.277\n",
       "2011-08-01 15:05:30.877   2011-08-02 15:07:17.932\n",
       "2011-08-01 15:11:41.236   2011-08-02 15:12:42.641\n",
       "2011-08-01 15:26:40.200   2011-08-02 15:27:24.004\n",
       "                                    ...          \n",
       "2012-07-27 19:53:34.640   2012-07-30 06:13:28.136\n",
       "2012-07-27 19:57:45.879   2012-07-30 06:13:28.136\n",
       "2012-07-27 19:59:58.054   2012-07-30 06:13:28.136\n",
       "2012-07-27 20:01:25.794   2012-07-30 06:13:28.136\n",
       "2012-07-27 20:14:35.480   2012-07-30 06:13:28.136\n",
       "Name: date_time, Length: 9515, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute vertical barrier\n",
    "vertical_barriers = ml.labeling.add_vertical_barrier(t_events=cusum_events, close=data['close'], num_days=1)\n",
    "vertical_barriers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Take Profit and Stop Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_sl = [1,2]\n",
    "min_ret = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Triple-barrier Events using CUSUM filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-07 15:06:07.878537 100.0% apply_pt_sl_on_t1 done after 0.13 minutes. Remaining 0.0 minutes..\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ret</th>\n",
       "      <th>trgt</th>\n",
       "      <th>bin</th>\n",
       "      <th>side</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-08-02 19:31:14.387</th>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.005069</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-08-02 19:42:30.586</th>\n",
       "      <td>0.006422</td>\n",
       "      <td>0.006108</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-08-02 19:45:56.176</th>\n",
       "      <td>0.007222</td>\n",
       "      <td>0.006424</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-08-02 19:50:37.185</th>\n",
       "      <td>0.009869</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-08-02 19:52:57.011</th>\n",
       "      <td>0.009063</td>\n",
       "      <td>0.007127</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-27 19:53:34.640</th>\n",
       "      <td>-0.001991</td>\n",
       "      <td>0.005877</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-27 19:57:45.879</th>\n",
       "      <td>-0.001268</td>\n",
       "      <td>0.005763</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-27 19:59:58.054</th>\n",
       "      <td>-0.001810</td>\n",
       "      <td>0.005655</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-27 20:01:25.794</th>\n",
       "      <td>-0.002713</td>\n",
       "      <td>0.005590</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-27 20:14:35.480</th>\n",
       "      <td>-0.002893</td>\n",
       "      <td>0.005528</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7821 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              ret      trgt  bin  side\n",
       "2011-08-02 19:31:14.387  0.005400  0.005069    1  -1.0\n",
       "2011-08-02 19:42:30.586  0.006422  0.006108    1  -1.0\n",
       "2011-08-02 19:45:56.176  0.007222  0.006424    1  -1.0\n",
       "2011-08-02 19:50:37.185  0.009869  0.006789    1  -1.0\n",
       "2011-08-02 19:52:57.011  0.009063  0.007127    1  -1.0\n",
       "...                           ...       ...  ...   ...\n",
       "2012-07-27 19:53:34.640 -0.001991  0.005877    0   1.0\n",
       "2012-07-27 19:57:45.879 -0.001268  0.005763    0   1.0\n",
       "2012-07-27 19:59:58.054 -0.001810  0.005655    0   1.0\n",
       "2012-07-27 20:01:25.794 -0.002713  0.005590    0   1.0\n",
       "2012-07-27 20:14:35.480 -0.002893  0.005528    0   1.0\n",
       "\n",
       "[7821 rows x 4 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple_barrier_events = ml.labeling.get_events(close=data['close'],\n",
    "                                                t_events=cusum_events,\n",
    "                                                pt_sl=pt_sl,\n",
    "                                                #check between hourly and daily vol\n",
    "                                                target=daily_vol,\n",
    "                                                min_ret=min_ret,\n",
    "                                                num_threads=3,\n",
    "                                                vertical_barrier_times=vertical_barriers,\n",
    "                                                side_prediction=data['side'],\n",
    "                                                verbose=True\n",
    "                                                )\n",
    "\n",
    "labels = ml.labeling.get_bins(triple_barrier_events, data['close'])\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of the Primary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables for analysis\n",
    "primary_forecast = pd.DataFrame(labels['bin'])\n",
    "primary_forecast['pred'] = 1\n",
    "primary_forecast.columns = ['actual', 'pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance metrics\n",
    "actual = primary_forecast['actual']\n",
    "pred = primary_forecast['pred']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLassification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      4659\n",
      "           1       0.40      1.00      0.58      3162\n",
      "\n",
      "    accuracy                           0.40      7821\n",
      "   macro avg       0.20      0.50      0.29      7821\n",
      "weighted avg       0.16      0.40      0.23      7821\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('CLassification Report')\n",
    "print(sk.metrics.classification_report(y_true=actual,  y_pred=pred, zero_division=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0, 4659],\n",
       "       [   0, 3162]], dtype=int64)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Confusion Matrix')\n",
    "sk.metrics.confusion_matrix(actual, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.40429612581511315"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Accuracy')\n",
    "sk.metrics.accuracy_score(actual, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Meta Model (Train Forest for bet size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get log of returns\n",
    "raw_data['log_ret'] = np.log(raw_data['close']).diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create momentum columns\n",
    "raw_data['mom1'] = raw_data['close'].pct_change(periods=1)\n",
    "raw_data['mom2'] = raw_data['close'].pct_change(periods=2)\n",
    "raw_data['mom3'] = raw_data['close'].pct_change(periods=3)\n",
    "raw_data['mom4'] = raw_data['close'].pct_change(periods=4)\n",
    "raw_data['mom5'] = raw_data['close'].pct_change(periods=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create volatility columns\n",
    "raw_data['volatility_50'] = raw_data['log_ret'].rolling(window=50, min_periods=50, center=False).std()\n",
    "raw_data['volatility_31'] = raw_data['log_ret'].rolling(window=31, min_periods=31, center=False).std()\n",
    "raw_data['volatility_15'] = raw_data['log_ret'].rolling(window=15, min_periods=15, center=False).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serial Correlation (Takes about 4 minutes)\n",
    "window_autocorr = 50\n",
    "\n",
    "raw_data['autocorr_1'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=1), raw=False)\n",
    "raw_data['autocorr_2'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=2), raw=False)\n",
    "raw_data['autocorr_3'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=3), raw=False)\n",
    "raw_data['autocorr_4'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=4), raw=False)\n",
    "raw_data['autocorr_5'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=5), raw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serial Correlation (Takes about 4 minutes)\n",
    "window_autocorr = 50\n",
    "\n",
    "raw_data['autocorr_1'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=1), raw=False)\n",
    "raw_data['autocorr_2'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=2), raw=False)\n",
    "raw_data['autocorr_3'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=3), raw=False)\n",
    "raw_data['autocorr_4'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=4), raw=False)\n",
    "raw_data['autocorr_5'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=5), raw=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompute sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['side'] = np.nan\n",
    "long_signals = raw_data['fast_mavg'] >= raw_data['slow_mavg']\n",
    "short_signals = raw_data['fast_mavg'] < raw_data['slow_mavg']\n",
    "raw_data.loc[long_signals, 'side'] = 1\n",
    "raw_data.loc[short_signals, 'side'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove look ahead bias\n",
    "raw_data = raw_data.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features at event times\n",
    "x = raw_data.loc[labels.index, :]\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unwanted columns\n",
    "x.drop(['open', 'high', 'low', 'close', 'cum_vol', 'cum_dollar', 'cum_ticks','fast_mavg', 'slow_mavg'], axis=1, inplace=True)\n",
    "y = labels['bin']\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the y values (0, 1)\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will strart by seperating our dataset in a training and test set for the purpose of implementing our random forest and checking itts abiility to accurately make predictions on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.dropna()\n",
    "x_training_validation = x\n",
    "y_training_validation = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create train, test, and split data from scikit-learn\n",
    "x_train, x_validate, y_train, y_validate = sk.model_selection.train_test_split(\n",
    "                                        x_training_validation, \n",
    "                                        y_training_validation,\n",
    "                                        test_size=0.70,\n",
    "                                        train_size=0.30,  \n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train dataframe\n",
    "train_df = pd.concat([y_train,x_train], axis=1, join='inner')\n",
    "train_df['bin'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsample training data for 50/50 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority = train_df[train_df['bin'] == 0]\n",
    "minority = train_df[train_df['bin'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority = train_df[train_df['bin'] == 0]\n",
    "minority = train_df[train_df['bin'] == 1]\n",
    "\n",
    "new_minority = sk.utils.resample(minority,\n",
    "                        replace=True,\n",
    "                        n_samples=majority.shape[0], # to match majority class\n",
    "                        random_state=42) # figure random state out\n",
    "\n",
    "train_df = pd.concat([majority, new_minority])\n",
    "train_df = sk.utils.shuffle(train_df, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['bin'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['bin']\n",
    "x_train = train_df.loc[:, train_df.columns != 'bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "parameters = {'max_depth': [2, 3, 4, 5, 7],\n",
    "                'n_estimators': [1, 10, 25, 50, 100],\n",
    "                'random_state': [42] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search(x_data, y_data):\n",
    "        rf = sk.ensemble.RandomForestClassifier()\n",
    "\n",
    "        clf = sk.model_selection.GridSearchCV(rf, parameters, cv=4, scoring='roc_auc', n_jobs=3)\n",
    "\n",
    "        clf.fit(x_data, y_data)\n",
    "\n",
    "        print(clf.cv_results_['mean_test_score'])\n",
    "        print(clf.best_params_['n_estimators'])\n",
    "        print(clf.best_params_['max_depth'])\n",
    "\n",
    "        return clf.best_params_['n_estimators'], clf.best_params_['max_depth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimator, depth = perform_grid_search(x_train, y_train)\n",
    "c_random_state = 42\n",
    "print('n_estimator: ' + str(n_estimator), '\\ndepth: ' + str(depth), '\\nc_random_state: ' + str(c_random_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refit new model with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = sk.ensemble.RandomForestClassifier(max_depth=depth, n_estimators=n_estimator, random_state=c_random_state)\n",
    "rf.fit(x_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rf.predict_proba(x_train)[:, 1]\n",
    "y_pred = rf.predict(x_train)\n",
    "fpr_rf, tpr_rf, _ = sk.metrics.roc_curve(y_train, y_pred_rf)\n",
    "print('Classification Report')\n",
    "print(sk.metrics.classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix')\n",
    "print(sk.metrics.confusion_matrix(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy')\n",
    "print(sk.metrics.accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda784a393941bc4e35b7c87680a7ceede7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
